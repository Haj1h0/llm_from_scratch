# 트랜스포머의 셀프 어텐션은 시퀀스에 있는 각 위치가 동일 시퀀스에 있는 모든 다른 위치와 상호 작용하여 관련성을 결정함으로써 입력 표현을 향상시키는 기술

# 실제 Transformer에서는 Key를 “임베딩 그대로” 쓰지 않는다.
# 반드시 가중치 행렬 W_K를 곱해서 새로운 표현 K를 만든다. 또한 Key(K)는 항상 위치 정보가 포함된 X에서 만들어진다.

# 실무에서는
# Q = inputs              # (6, 3)
# K = inputs              # (6, 3)
# scores = Q @ K.T        # (6, 6) # @ = 텐서 간 행렬 곱(벡터면 내적)

## 어텐션 스코어 정규화에 소프트맥스를 쓰는 이유1. 확률 분포로의 변환 (가중치 해석)어텐션 스코어는 임의의 실수 값을 가집니다. 이를 소프트맥스에 통과시키면 모든 요소의 합이 **1(100%)**이 되며, 각 값은 $0$과 $1$ 사이의 값으로 변환됩니다.이를 통해 모델은 "어떤 단어(토큰)에 몇 %의 비중을 두어 정보를 가져올지"를 명확하게 결정할 수 있습니다.2. 수치적 안정성과 지수 함수($e^x$)의 특성소프트맥스는 내부적으로 지수 함수를 사용하므로, 스코어 간의 미세한 차이를 크게 증폭시키는 효과가 있습니다.가장 관련이 높은 요소에는 큰 가중치를, 관련 없는 요소에는 $0$에 가까운 아주 작은 가중치를 부여하여 정보를 효율적으로 필터링합니다.3. 미분 가능한 연결 (학습의 용이성)소프트맥스는 모든 지점에서 매끄럽게 미분 가능합니다.역전파(Backpropagation) 과정에서 그레이디언트(Gradient)가 잘 전달되도록 설계되어 있어, 모델이 어텐션 가중치를 어떤 방향으로 수정해야 할지 학습하는 데 매우 유리한 속성을 제공합니다.4. 극한 값 제어 (Scaled Dot-Product)실전(특히 Transformer)에서는 단순히 소프트맥스만 쓰지 않고, 입력 값의 차원($\sqrt{d_k}$)으로 나누어주는 Scaling을 곁들입니다.이는 스코어가 너무 커져서 소프트맥스의 그래디언트가 $0$에 가까워지는 '포화 상태(Saturation)'를 방지하기 위함입니다.### 요약하자면소프트맥스는 단순한 정규화를 넘어, 비중의 차이를 극대화하고 **학습 가능한 매끄러운 기울기(Gradient)**를 제공하기 때문에 어텐션 메커니즘의 표준으로 자리 잡았습니다.혹시 이 과정에서 등장하는 **'스케일드 닷 프로덕트 어텐션(Scaled Dot-Product Attention)'**의 수식이 궁금하신가요? 원하시면 관련 수식과 함께 더 깊게 설명해 드릴 수 있습니다.

python# Query, Key, Value 행렬로 변환
Q = inputs @ W_q  # Query 변환
K = inputs @ W_k  # Key 변환
V = inputs @ W_v  # Value 변환

attn_scores = Q @ K.T / sqrt(d_k)  # Scaled dot-product

python# 1. Query와 Key의 내적 → Attention Score
attn_scores = Q · K^T

# 2. Softmax로 정규화 → Attention Weight (확률 분포)
attn_weights = softmax(attn_scores)

# 3. Attention Weight와 Value의 가중합 → Context Vector
context_vector = attn_weights · V

-> 클로드 마지막 질문 
-> 노션 https://www.notion.so/hajiho123123/a-2dbba5a0491e807e906eef5fef98accf

구체적 예시
단일 쿼리 ("journey") 기준
python# Attention Weights (확률 분포)
attn_weights_2 = [0.1386, 0.2379, 0.2331, 0.1237, 0.1082, 0.1581]
#                  Your   journey starts  with    one     step

# Values (입력 토큰 임베딩)
values = inputs = [
    [0.43, 0.15, 0.89],  # Your
    [0.55, 0.87, 0.66],  # journey
    [0.57, 0.85, 0.64],  # starts
    [0.22, 0.58, 0.33],  # with
    [0.77, 0.25, 0.10],  # one
    [0.05, 0.80, 0.55]   # step
]

# Context Vector (가중 평균)
context_vec_2 = 0.1386 × [0.43, 0.15, 0.89]  # Your의 기여
              + 0.2379 × [0.55, 0.87, 0.66]  # journey의 기여 (가장 큼!)
              + 0.2331 × [0.57, 0.85, 0.64]  # starts의 기여
              + 0.1237 × [0.22, 0.58, 0.33]  # with의 기여
              + 0.1082 × [0.77, 0.25, 0.10]  # one의 기여
              + 0.1581 × [0.05, 0.80, 0.55]  # step의 기여
              = [0.4419, 0.6515, 0.5683]
행렬 형태 (모든 토큰)
python# Attention Weights: [6, 6]
attn_weights = [
    [0.19, 0.18, 0.18, 0.12, 0.11, 0.13],  # Your
    [0.14, 0.24, 0.23, 0.12, 0.10, 0.15],  # journey
    [0.14, 0.24, 0.23, 0.12, 0.10, 0.15],  # starts
    [0.16, 0.19, 0.19, 0.14, 0.13, 0.16],  # with
    [0.16, 0.20, 0.20, 0.13, 0.18, 0.14],  # one
    [0.15, 0.23, 0.23, 0.14, 0.12, 0.19]   # step
]

# Values: [6, 3]
values = [
    [0.43, 0.15, 0.89],
    [0.55, 0.87, 0.66],
    [0.57, 0.85, 0.64],
    [0.22, 0.58, 0.33],
    [0.77, 0.25, 0.10],
    [0.05, 0.80, 0.55]
]

# Context Vectors: [6, 3]
context_vectors = attn_weights @ values
#                 ─────────────   ──────
#                 [6, 6]          [6, 3] → [6, 3]
행렬 곱셈으로 한번에
python# 각 행이 하나의 context vector
context_vectors = torch.matmul(attn_weights, values)

# 결과:
# context_vectors[0] = "Your"의 새로운 표현
# context_vectors[1] = "journey"의 새로운 표현
# context_vectors[2] = "starts"의 새로운 표현
# ...
```

## 시각화
```
         Attention Weights              Values            Context Vector
              [6, 6]          ×         [6, 3]       =        [6, 3]

┌─────────────────────┐   ┌─────────────────┐   ┌─────────────────┐
│ 0.19 0.18 0.18 ... │ × │ 0.43 0.15 0.89 │ = │ 0.44 0.59 0.64 │
│ 0.14 0.24 0.23 ... │   │ 0.55 0.87 0.66 │   │ 0.44 0.65 0.57 │
│ 0.14 0.24 0.23 ... │   │ 0.57 0.85 0.64 │   │ 0.44 0.65 0.56 │
│ 0.16 0.19 0.19 ... │   │ 0.22 0.58 0.33 │   │ 0.43 0.62 0.60 │
│ 0.16 0.20 0.20 ... │   │ 0.77 0.25 0.10 │   │ 0.45 0.61 0.58 │
│ 0.15 0.23 0.23 ... │   │ 0.05 0.80 0.55 │   │ 0.44 0.65 0.58 │
└─────────────────────┘   └─────────────────┘   └─────────────────┘
    (확률 분포)              (원본 정보)          (문맥화된 표현)
Self-Attention vs General Attention
Self-Attention (현재 예제)
pythonQuery = inputs
Key = inputs      # 같은 입력!
Value = inputs    # 같은 입력!
General Attention (실제 Transformer)
pythonQuery = inputs @ W_Q  # 학습 가능한 변환
Key = inputs @ W_K    # 학습 가능한 변환
Value = inputs @ W_V  # 학습 가능한 변환

# Value는 Key와 다른 표현일 수 있음!