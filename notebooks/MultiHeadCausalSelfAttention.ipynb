{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Haj1h0/llm-from-scratch-code/blob/main/MultiHeadCausalSelfAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DMsr_bgHb8t"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, context_length, dropout=0.0, qkv_bias=False):\n",
    "        \"\"\"\n",
    "        d_model: 모델 차원 (임베딩 차원)\n",
    "        num_heads: 헤드 수\n",
    "        context_length: 최대 시퀀스 길이 (마스크 크기)\n",
    "        dropout: 어텐션 가중치에 적용할 드롭아웃\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model은 num_heads로 나누어 떨어져야 합니다.\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Q, K, V를 한 번에 뽑는 선형층 (실무에서도 이 패턴 많이 씀)\n",
    "        self.W_qkv = nn.Linear(d_model, 3 * d_model, bias=qkv_bias)\n",
    "\n",
    "        # 여러 헤드 출력(concat) -> 다시 d_model로 투영하는 출력 projection\n",
    "        self.W_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 어텐션 가중치 드롭아웃\n",
    "        self.attn_dropout = nn.Dropout(dropout\n",
    "        # 출력 드롭아웃 (원하면 빼도 됨)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 상삼각 마스크 (미래 토큰 가리기) - [context_length, context_length]\n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask)  # 학습 파라미터는 아닌 버퍼로 등록\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, num_tokens, d_model)\n",
    "        return: (batch, num_tokens, d_model)\n",
    "        \"\"\"\n",
    "        b, n, d = x.shape\n",
    "        assert d == self.d_model\n",
    "\n",
    "        # 1) Q, K, V 한 번에 계산\n",
    "        #    qkv: (b, n, 3 * d_model)\n",
    "        qkv = self.W_qkv(x)\n",
    "        # 2) q, k, v로 쪼개기\n",
    "        #    각각 (b, n, d_model)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # 3) 멀티헤드 형태로 reshape\n",
    "        #    (b, n, num_heads, head_dim) -> (b, num_heads, n, head_dim)\n",
    "        q = q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # q, k, v: (b, h, n, d_head)\n",
    "\n",
    "        # 4) 어텐션 스코어: Q K^T / sqrt(d_head)\n",
    "        #    (b, h, n, d_head) @ (b, h, d_head, n) -> (b, h, n, n)\n",
    "        attn_scores = q @ k.transpose(-2, -1)\n",
    "        attn_scores = attn_scores / math.sqrt(self.head_dim)\n",
    "\n",
    "        # 5) Causal 마스크 적용\n",
    "        #    self.mask: (context_length, context_length)\n",
    "        #    현재 시퀀스 길이 n만큼 잘라쓰기\n",
    "        #    True인 위치에 -inf 채워서 softmax 후 0 되게 함\n",
    "        causal_mask = self.mask[:n, :n].bool()  # (n, n)\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n",
    "\n",
    "        # 6) 소프트맥스 + 드롭아웃\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)  # (b, h, n, n)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # 7) 문맥벡터 계산: (b, h, n, n) @ (b, h, n, d_head) -> (b, h, n, d_head)\n",
    "        context = attn_weights @ v  # (b, h, n, d_head)\n",
    "\n",
    "        # 8) 헤드 합치기: (b, h, n, d_head) -> (b, n, h * d_head = d_model)\n",
    "        context = context.transpose(1, 2).contiguous()  # (b, n, h, d_head)\n",
    "        context = context.view(b, n, self.d_model)      # (b, n, d_model)\n",
    "\n",
    "        # 9) 최종 선형 변환 + 드롭아웃\n",
    "        out = self.W_out(context)        # (b, n, d_model)\n",
    "        out = self.proj_dropout(out)\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPyzvES+gum1SXCj9ESXoAE",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
