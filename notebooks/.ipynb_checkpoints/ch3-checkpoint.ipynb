{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Haj1h0/llm-from-scratch-code/blob/main/ch3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2egp6PVVobF6",
    "outputId": "f22b8755-c7cd-4ac9-9817-ed0c56a98395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이토치 버전: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"파이토치 버전:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vwyq7sY1o9vc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(  # 3차원 벡터로 임베딩한 입력 시퀀스\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIX9mJJDqJ8L",
    "outputId": "ff6d75d7-4d3f-48d8-83f0-5c2ed6457d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# 트랜스포머의 Self-Attention은 시퀀스에 있는 각 위치가 동일 시퀀스에 있는 모든 다른 위치와 상호 작용하여 관련성을 결정함으로써 입력 표현을 향상시킨다.\n",
    "# inputs[1]인 두 번째 입력 토큰 \"journey\"와 모든 입력 토큰들(자기 자신 포함) 사이에서\n",
    "# 점곱을 계산하여 정규화되지 않은(softmax를 적용하지 않은) 어텐션 점수를 구하는 코드\n",
    "# inputs[1]이 query인 것은 sample일 뿐, Self-Attention에서는 모든 단어가 차례로 query가 되어 다른 단어들과의 관계를 계산한다.\n",
    "# xᵢ와 query(= x²)의 방향이 비슷할수록 점곱이 크고 유사도가 높다. → attention score ↑\n",
    "# 방향이 다르면 점곱이 작고 유사도가 낮다. → attention score ↓\n",
    "# attention_scores = Q · Kᵀ\n",
    "\n",
    "query = inputs[1]  \n",
    "\n",
    "# inputs.shape = torch.Size([6, 3]) # 6개의 단어, 각각 3차원 벡터로 표현\n",
    "# inputs.shape[0] → 6 (첫 번째 차원의 크기), inputs.shape[1] → 3 (두 번째 차원의 크기) 즉, shape[0]: 행의 개수, shape[1]: 열의 개수\n",
    "# torch.empty(6) = [쓰레기값, 쓰레기값, 쓰레기값, 쓰레기값, 쓰레기값, 쓰레기값] 즉, attention 점수 저장용 1차원 텐서 공간\n",
    "# ex) # i=3: attn_scores_2[3] = 0.22*0.55 + 0.58*0.87 + 0.33*0.66 = 0.8414\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) \n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # 1차원 벡터이므로 전치 X\n",
    "# 질문하신 내용처럼 torch.dot은 오직 1차원 텐서에만 사용 가능하기 때문에 순서가 바뀌어도 연산 결과(스칼라 값)가 동일한 것이고,\n",
    "# 나중에 차원이 높아지면 **@ (행렬 곱)**를 사용하는 것이 정석입니다.\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfDj3MgjupuH",
    "outputId": "c9b1bb87-52a9-4c64-aeb3-72a42cbc78c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# calculating the attention score of query inputs[0]\n",
    "\n",
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]  # == torch.dot(inputs[0], query)\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3efB67MwZ8Q",
    "outputId": "846296f6-76e4-4b4d-c310-b317563d6dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어텐션 가중치: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "합: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# normalizes the attention score to a sum of 1\n",
    "# attn_scores_2 / attn_scores_2.sum()을 통한 normalization은 음수 발생 시 확률 분포의 성질을 만족하지 못한다.\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"어텐션 가중치:\", attn_weights_2_tmp)\n",
    "print(\"합:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoZs2EJcwyap",
    "outputId": "dd535cad-80c8-49e8-a2f9-283f1091c1cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어텐션 가중치: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "합: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Naive function with direct softmax implementation \n",
    "# softmax(si) = exp(si) / Σ exp(sj)\n",
    "# sum = 1, 모든 값은 0~1 사이, 큰 값은 더 크게 만들어주고 (확실성↑), 작은 값은 더 작아짐 (불확실성↓)\n",
    "# 이는 attention이 더 날카롭고 선명해져 '확률 분포’를 만들기에 매우 적합하다.\n",
    "# 해당 함수는 입력이 매우 작거나 클 경우 언더플로우 혹은 오버플로우가 발생할 수 있기에 수치적으로 불안정하다.\n",
    "# 큰 값에 exp()를 적용하면 오버플로우 발생 가능, 작은 값에 exp()를 적용하면 언더플로우 발생 가능\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"어텐션 가중치:\", attn_weights_2_naive)\n",
    "print(\"합:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGakLXIWxEvY",
    "outputId": "49360f8e-74ff-492f-87c9-c6fc60f98b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어텐션 가중치: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "합: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# torch.softmax()\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"어텐션 가중치:\", attn_weights_2)\n",
    "print(\"합:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8k44grvyeqz",
    "outputId": "c44fa059-29f5-409c-abaa-87a8edb66d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# context vector 계산\n",
    "# \"journey\" 토큰이 다른 모든 토큰들의 정보를 가중평균하여 새로운 표현을 만드는 코드\n",
    "\n",
    "query = inputs[1] \n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)  # [0.00, 0.00, 0.00], query와 같은 shape인 3차원 영벡터\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i  # scala X vector = “1행 3열” 또는 “3차원 벡터”\n",
    "\n",
    "# 문맥 벡터 = Σ (어텐션 가중치 × 입력 벡터)\n",
    "# context_vec_2 = 0.1386 × [0.43, 0.15, 0.89]  (\"Your\")\n",
    "#              + 0.2379 × [0.55, 0.87, 0.66]  (\"journey\") ← 가장 큰 영향!\n",
    "#              + 0.2331 × [0.57, 0.85, 0.64]  (\"starts\")\n",
    "#              + 0.1237 × [0.22, 0.58, 0.33]  (\"with\")\n",
    "#              + 0.1082 × [0.77, 0.25, 0.10]  (\"one\")\n",
    "#              + 0.1581 × [0.05, 0.80, 0.55]  (\"step\")\n",
    "#              ─────────────────────────────────────\n",
    "#              = [0.4419, 0.6515, 0.5683]\n",
    "# 6개의 3차원 입력 벡터에 어텐션 가중치를 적용하여 가중 평균을 구함으로써, 주변 토큰의 정보를 유기적으로 흡수한 단일 문맥 벡터(Context Vector)\n",
    "print(context_vec_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_4ooN9-0Fac",
    "outputId": "cfd64aea-20ce-45e2-c67e-c0cd533f7f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# 모든 입력 토큰 쌍에 대해 attention score를 계산\n",
    "\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):  # Query\n",
    "    for j, x_j in enumerate(inputs):  # Key\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)  # attn_scores[1, :] == attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCKfP0yyt3r4",
    "outputId": "9892bf9a-5811-4a1c-ae48-7c4a6ae25019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T  # [6,3] @ [3,6] = [6,6]\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myf66uN1t-Ly",
    "outputId": "e68cc3b9-329a-4f05-9a48-0f0243bc1353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# 각 행의 합이 1이 되도록 정규화\n",
    "\n",
    "# dim = -1 은 \"마지막 차원, 축(axis)\"을 의미한다. 즉, 텐서의 가장 오른쪽 축을 기준으로 연산하겠다는 뜻이다. \n",
    "# dim=1 또는 dim=-1: 행 방향으로 softmax (각 행의 합 = 1), dim=0: 열 방향으로 softmax (각 열의 합 = 1)\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1) \n",
    "\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DnxEqeAuNMg",
    "outputId": "532cacd9-e2e3-40d1-8c20-877f233582ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 번째 행의 합: 1.0\n",
      "모든 행의 합: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"두 번째 행의 합:\", row_2_sum)\n",
    "\n",
    "print(\"모든 행의 합:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83IMvmqVuOIK",
    "outputId": "6165c96f-483b-4bc7-8108-eebaa0faa550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# context vector = attention weight * 값(Value)\n",
    "\n",
    "all_context_vecs = attn_weights @ inputs # [6,6] @ [6,3] = [6,3]\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmNu0jZPweSE",
    "outputId": "b1d59219-6580-4e70-b3d8-19d39ec43276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전에 계산한 두 번째 문맥 벡터: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"이전에 계산한 두 번째 문맥 벡터:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_Gsi5xbdwhf_"
   },
   "outputs": [],
   "source": [
    "# 훈련 가능한 가중치를 가진 셀프 어텐션 구현하기\n",
    "# 셀프 어텐션 메커니즘을 구현하기 위해 세 개의 훈련 가중치 행렬 Wq, Wk, Wv를 도입한다.\n",
    "# 이 세 개의 행렬에 행렬 곱셈을 적용하여 임베딩된 입력 토큰 Xi을 쿼리, 키, 값 벡터로 투영합니다.\n",
    "# X = TokenEmbedding + PositionEmbedding\n",
    "# 쿼리 벡터 qi = xi * Wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwCP5TDOQYNN",
    "outputId": "331dedfb-1524-4916-9f31-c8327794f21a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT 모델에서는 입력과 출력 차원이 일반적으로 같습니다. 하지만 계산 과정을 설명하기 쉽도록 입력과 출력 차원을 다르게 하겠습니다.\n",
    "x_2 = inputs[1] # 두 번째 입력 원소, [0.55, 0.87, 0.66]\n",
    "d_in = inputs.shape[1] # 입력 임베딩 크기, d=3\n",
    "d_out = 2 # 출력 임베딩 크기, d=2\n",
    "x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정말 좋은 질문입니다! 결론부터 말씀드리면 **\"항상 동일한 것은 아니지만, 많은 모델에서 동일하게 설정(Default)하는 경우가 많다\"**가 정답입니다.보통 입력 임베딩의 크기를 $d_{model}$이라 하고, $Q, K, V$를 만들기 위한 가중치 행렬(Weight Matrix)의 결과 차원을 $d_k$라고 부릅니다.1. 차원이 동일한 경우 (예: 기본 Transformer)가장 유명한 논문인 Attention Is All You Need의 기준으로는 다음과 같습니다.입력 임베딩 ($d_{model}$): 512차원가중치 결과 ($d_k, d_v$): 512차원 (단, 멀티 헤드 어텐션으로 나뉘기 전 총합 기준)이 경우 입력 벡터의 길이와 출력 벡터의 길이가 같습니다. 이렇게 하면 잔차 연결(Residual Connection)을 할 때 입력값과 결과값을 더하기가 아주 편하기 때문에 기본값으로 많이 사용합니다.2. 차원이 다른 경우 (설계에 따라 가능)수학적으로는 가중치 행렬 $W_Q, W_K, W_V$의 크기를 조절하여 임베딩 차원을 늘리거나 줄일 수 있습니다.예를 들어, 입력 임베딩은 512차원인데 $Q, K, V$는 256차원으로 설계할 수 있습니다.이때 가중치 행렬의 크기는 $512 \\times 256$이 됩니다.3. 멀티 헤드 어텐션(Multi-Head Attention)에서의 비밀여기서 헷갈리기 쉬운 포인트가 하나 더 있습니다. 실제 모델은 하나의 큰 어텐션을 쓰는 게 아니라 여러 개로 쪼개서 씁니다.**전체 차원($d_{model}$)**이 512이고 **헤드(Head)**가 8개라면:각 헤드에서의 실제 $Q, K, V$ 벡터 크기는 $512 \\div 8 = 64$차원이 됩니다.즉, 개별 헤드 내부에서는 **입력(512) $\\neq$ 출력(64)**으로 크기가 다릅니다! 하지만 나중에 이 8개를 다시 합치면 원래의 512차원으로 돌아오는 구조입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Rn_cP2cMQbgu"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "# 가중치 행렬로 모델을 훈련하려면 requires_grad=True로 지정해서 훈련 과정 중에 이 행렬들을 업데이트해야 한다. 출력 간소화 위해 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQSeOjyJQwaA",
    "outputId": "21f569a0-98eb-4ed7-e955-4db3fbbf4580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query # 두 번째 입력 원소에 대한 값을 계산하므로 _2로 씁니다.\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)\n",
    "print(x_2.shape)\n",
    "print(W_query.shape)\n",
    "print(query_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQfazFJnQ0A3",
    "outputId": "13c941e9-59c5-431e-98e4-e22f8b976833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# 여섯 개의 입력 토큰을 3D에서 2D 임베딩 공간에 투영.\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoXFwJKOSROS",
    "outputId": "926f8212-8a90-45df-bcb3-f09f8d998127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] # 파이썬 인덱스는 0부터 시작합니다.\n",
    "# attn_score_22 = query_2.dot(keys_2)\n",
    "attn_score_22 = query_2 @ keys_2\n",
    "\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjh04\\AppData\\Local\\Temp\\ipykernel_10468\\2811642555.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4419.)\n",
      "  attn_score_22 = query_2 @ keys_2.T\n"
     ]
    }
   ],
   "source": [
    "# attn_score_22 = query_2 @ keys_2.T \n",
    "# 1차원 텐서에 “행렬용 전치(.T)”를 동일하게 적용하려고 하면 에러(또는 shape 불일치)가 나는 게 정상이야.\n",
    "# 더 좋은 코드로 줄이기 가능 GPT 질문 -> 2D로 맞추고 점곱하기? 최적?\n",
    "# print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eN_UV9oBVmeK",
    "outputId": "1a8e5a02-2895-4e06-9633-6611992b30fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # 주어진 쿼리에 대한 모든 어텐션 점수\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EYtVHoRVuqm",
    "outputId": "dc7adddf-0770-4101-a34a-b2efc7aa9a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# attention score -> attention weight, 이때 임베딩 차원의 제곱근 d_k ** 0.5 로 나누어 어텐션 점수의 스케일을 조정한다.\n",
    "# 임베딩 차원의 제곱근으로 나눈다” → 정확히는 키 벡터의 차원 dₖ의 제곱근(√dₖ)\n",
    "# 나누는 이유로는 점곱(dot product)이 차원이 커질수록 너무 커지기 때문이다. 값이 커질수록 softmax가 극단적으로 편향될 가능성이 있다.\n",
    "# 그럼 왜 하필 “제곱근(√dₖ)”으로 나누는가? 핵심은 점곱(dot product)의 분산이 dₖ에 비례해서 증가한다. 고로 제곱근(√dₖ)로 나누면 분산이 1로 보정된다.\n",
    "d_k = keys.shape[1]\n",
    "print(d_k)\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwcV0LUnVy8f",
    "outputId": "70f36c0a-cf70-4380-dab9-8d2cd35c7afd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFW-a8x3XJhi",
    "outputId": "2951eea6-7072-4784-d080-a610671d0fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 셀프 어텐션 파이썬 클래스 구현하기\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n",
    "\n",
    "# 입력 shapes:\n",
    "# Q: (batch, seq_len, head_dim)\n",
    "# K: (batch, seq_len, head_dim)\n",
    "# Kᵀ: (batch, head_dim, seq_len) -> K.transpose(-2, -1)\n",
    "# K.T는 2차원 텐서(행렬)일 때만 안전하게 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "_8JRCfJ7Xasr",
    "outputId": "04db5169-0da7-4ec2-f0e7-a11ff7557dc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass SelfAttention(nn.Module):\\n    def __init__(self, d_model, d_k):\\n        super().__init__()\\n        self.W_Q = nn.Parameter(torch.randn(d_model, d_k))\\n        self.W_K = nn.Parameter(torch.randn(d_model, d_k))\\n        self.W_V = nn.Parameter(torch.randn(d_model, d_k))\\n\\n    def forward(self, x):\\n        Q = x @ self.W_Q\\n        K = x @ self.W_K\\n        V = x @ self.W_V\\n\\n        scores = Q @ K.transpose(-2, -1) / math.sqrt(K.size(-1)) # K.size(-1) = K 텐서의 마지막(last) 차원의 크기를 가져온다는 의미 = = key 벡터의 차원 d_k\\n        attn = scores.softmax(dim=-1)\\n        out = attn @ V\\n        return out\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Parameter(torch.randn(d_model, d_k))\n",
    "        self.W_K = nn.Parameter(torch.randn(d_model, d_k))\n",
    "        self.W_V = nn.Parameter(torch.randn(d_model, d_k))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = x @ self.W_Q\n",
    "        K = x @ self.W_K\n",
    "        V = x @ self.W_V\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(K.size(-1)) # K.size(-1) = K 텐서의 마지막(last) 차원의 크기를 가져온다는 의미 = = key 벡터의 차원 d_k\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        out = attn @ V\n",
    "        return out\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f9qlbF7YmBI",
    "outputId": "3417b4dd-dc08-4fc2-cb53-13bc6634fc12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# nn.Parameter(torch.rand(...) 대신에 nn.Linear를 사용하는 또 다른 큰 장점은 nn.Linear는 안정적으로 모델을 훈련하는데 도움이 되는 가중치 초기화를 제공한다.\n",
    "# nn.Linear(in, out)는 내부에서 Xavier(Glorot) 초기화를 자동 적용하여 일정한 통계적 기준에 따라 “좋은 초기값”으로 설정됨, 학습 안정성 ↑\n",
    "# nn.Parameter(torch.randn(...))은 정규분포(평균=0, 표준편차=1)로 초기화됨, 분포는 N(0, 1), 가중치 크기가 nn.Linear보다 더 크거나 다양할 수 있음, 학습 안정성은 스스로 책임지는 방식\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Md-Wqh8YpF4",
    "outputId": "538de126-2523-41f3-8a6e-7831e12cb1fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 코잘 어텐션으로 미래의 단어를 감추기\n",
    "# 코잘 어텐션에서 주대각선 위의 어텐션 가중치를 마스크하여 주어진 입력에 대해 어텐션 가중치로 문맥 벡터를 계산하는 동안 미래 토큰을 엿볼수 없게 한다.\n",
    "# 코잘 어텐션을 사용하지 않을 경우, 모델은 실제 생성 과정에서는 이용할 수 없는 정보를 사용하게 됨\n",
    "# 즉, 훈련 환경과 추론 환경이 달라짐 → 모델이 망함 -> 이를 **정보 누수(data leakage)**라고 한다.\n",
    "# 편의상 이전 절에서 만든 SelfAttention_v2 객체의 쿼리와 키 가중치 행렬을 재사용.\n",
    "\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJrEHZyraoyV",
    "outputId": "698651a9-e3ea-4f6e-85d2-4b61700e11e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 미래 어텐션 가중치를 마스킹하는 가장 간단한 방법은 파이토치 tril 함수로 주대각선과 그 아래의 원소는 1, 주대각선 위의 원소는 0인 마스크를 만드는 것.\n",
    "# torch.tril()을 사용하면 주대각선 기준으로 위쪽을 0으로 만드는 마스크 텐서 생성.\n",
    "\n",
    "# 어텐션 스코어 행렬의 크기는 (seq_len × seq_len)\n",
    "# 행(row, i) = 현재 \"쿼리 토큰\" (i번째 토큰)\n",
    "# 열(column, j) = 보고 싶은 \"키 토큰\" (j번째 토큰)\n",
    "# 즉, 행 i는 “i번째 토큰이 누구를 참고하냐”, 열 j는 “j번째 토큰을 참고할지 말지”\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) # diagonal=-1, triu()\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtHFO-cBbA7u",
    "outputId": "516c6931-1d8e-4f7a-f605-dffed716c485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple  # torch.tril(attn_weights)와 동일\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OjGaSIYvbSFU",
    "outputId": "2b0b66fc-f293-4777-d70d-eca7a018f8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 하지만 위와 같이 마스크를 소프트맥스 이후에 적용하면 소프트맥스로 만든 확률 분포가 어그러집니다.\n",
    "# 실무에서는 스케일링 → 마스크 → softmax 이 순서가 정석이며 가장 안정적. 마스킹 후 스케일을 한다면, -inf / √d_k 같은 쓸데없는 연산 발생\n",
    "# masked_fill : scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "# cf) 소프트 맥스 사용 이유 : 단순 정규화하면 \"평평한 분포\"가 나와서 Attention의 날카로운 선택 능력이 사라짐. 고로 attention은 큰 score를 더 중요하게 보고, 작은 score는 훨씬 덜 보게 되어야 함\n",
    "\n",
    "# 각 행의 합이 1이 되도록 만들기 위해 어텐션 가중치를 다음과 같이 정규화할 수 있습니다.\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True) # 마지막 차원(dim=-1)을 따라 각 row 합을 구함\n",
    "# keepdim=True는 shape을 (batch, seq_len, 1)처럼 유지하기 위함 → 그래야 나중에 broadcasting으로 나누기 가능\n",
    "# broadcasting은 서로 다른 shape의 텐서끼리 연산할 때, PyTorch가 작은 텐서를 자동으로 큰 텐서에 맞게 확장해주는 규칙이다.\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WMX7_dJft6g",
    "outputId": "b28f6e03-2127-4390-873c-97ec3ba4e5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 함수에 들어가기 전에 정규화되지 않은 어텐션 점수를 음의 무한대로 마스킹\n",
    "# torch.tril() → Lower triangular (아래 삼각형)\n",
    "# torch.triu() → Upper triangular (위 삼각형)\n",
    "\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf) # .masked_fill() = mask.bool()에서 True인 위치는 attn_scores 값을 -inf로 바꿔라, 즉, 첫 번째 인자 = 어느 위치를 바꿀지(Tensor of bool), 두 번째 인자 = 뭘로 바꿀지(value)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llVOSF3af18g",
    "outputId": "4d2f820f-73cc-4012-ba16-a8ea165e253a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "sT9zxIIeglAG"
   },
   "outputs": [],
   "source": [
    "# 드롭아웃으로 어텐션 가중치에 추가적으로 마스킹하기\n",
    "# 코잘(Causal) 마스킹은 언어모델이 오토리그레시브 구조를 지키도록 강제하는 제약 조건으로 과적합과는 관련 없음. 훈련/추론 consistency(일관성) 문제이다.\n",
    "# 과적합 방지는 드롭아웃(Dropout) 같은 regularization 기법으로 해결한다.\n",
    "# Self-Attention에서 Dropout은 softmax를 적용한 후에 적용한다.\n",
    "# QKᵀ → scaling → mask → softmax → dropout → V 곱하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOKuYNcihhbj",
    "outputId": "407e7636-c0ea-4c3f-9323-a9009b045c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 이 예시에서는 드롭아웃 비율을 50%으로 지정합니다. 어텐션 가중치의 절반을 랜덤하게 마스킹한다는 의미입니다. (나중에 GPT 모델을 훈련할 때는 0.1이나 0.2 정도의 낮은 드롭아웃 비율을 사용하겠습니다.\n",
    "\n",
    "# 드롭아웃 비율 0.5(50%)를 적용하면 드롭아웃되지 않은 값은 1/0.5=2배 만큼 크기가 증가할 것입니다.\n",
    "# 스케일 조정 배율은 1 / (1 - dropout_rate)와 같이 계산합니다.\n",
    "# 드롭아웃은 단순히 “값을 없애는(0으로 만드는)” 게 아닌 훈련 시 평균적 분포(scale)를 유지하기 위해 남은 값들을 1/(1-p) 만큼 키운다.\n",
    "# 이를 **“inverted dropout”**이라고 부른다.\n",
    "# 예로 값만 없애는 droupout 진행 시, dropout 전에 평균 = (2+3+5+7)/4 = 4.25, dropout 후 평균 = (2+5)/4 = 1.75 (절반 이하)로 모델의 출력을 계속 작게 만들어버림, 스케일이 변경된다.\n",
    "# 그래서 남아 있는 값들을 1 / (1 - p) 만큼 키워 원래 값과 대략 동일한 스케일을 유지한다.\n",
    "\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # 50% 드롭아웃 비율\n",
    "example = torch.ones(6, 6) # 1로 채워진 행렬을 만듭니다.\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maWSy6P_hsdm",
    "outputId": "33753698-0b82-4a90-f5fa-0b1133091aeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ma1cbJ3XliVw",
    "outputId": "a15a2cac-f4f3-47c3-b128-c4ac20adab5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# 코잘 어텐션 클래스 구현하기? 라기보다 배치 구현\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 각각 여섯 개의 토큰으로 구성된 두 개의 입력. 각 토큰의 임베딩 차원은 3입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xkgrczWo8p7E",
    "outputId": "d7360f46-9a8d-4a53-ba29-df7f35eff88d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # 추가\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # 추가\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # b: 배치 차원\n",
    "        # 입력의 `num_tokens`가 `context_length`를 넘는 경우 마스크 생성에서 오류가 발생합니다.\n",
    "        # 실제로는 forward 메서드에 들어오기 전에 LLM이 입력이 `context_length`를\n",
    "        # 넘지 않는지 확인하기 때문에 문제가 되지 않습니다.\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # 전치\n",
    "        attn_scores.masked_fill_(  # _ 메서드는 인플레이스 연산입니다.\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens`은 배치에 있는 토큰 개수가 문맥 길이보다 짧은 경우를 고려합니다.\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # 추가\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tjdwqb_X9gJU",
    "outputId": "a12056b9-85f9-4209-d537-8d136d38c49f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# 싱글 헤드 어텐션을 멀티 헤드 어텐션으로 확장하기\n",
    "# 멀티헤드는 각 헤드마다 \"다른 관점\"을 학습시키기 위해서 싱글 헤드를 여러 개 병렬 실행한 뒤 → concat → 마지막에 선형변환 한 번 더 해주는 구조다.\n",
    "\n",
    "# 1. 여러 개의 싱글 헤드 어텐션 층 쌓기\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # 토큰 개수\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "# 위 구현에서 임베딩 차원은 4입니다. 문맥 벡터는 물론 쿼리, 키, 값 벡터의 차원으로 d_out=2를 지정했기 때문입니다. 두 개의 어텐션 헤드가 있으므로 출력 임베딩 차원은 2*2=4가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yr3cf75I_Qg9",
    "outputId": "da8c983e-42e8-4afb-e613-24f65f4f1527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# 2.  가중치 분할로 멀티 헤드 어텐션 구현하기\n",
    "# 대신에 하나의 W_query, W_key, W_value 가중치 행렬을 만든다음 개별 어텐션 헤드를 위해 이 가중치를 개별 행렬로 분할합니다.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out은 num_heads로 나누어 떨어져야 합니다\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # 원하는 출력 차원에 맞도록 투영 차원을 낮춥니다.\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear 층을 사용해 헤드의 출력을 결합합니다.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # `CausalAttention`과 마찬가지로, 입력의 `num_tokens`가 `context_length`를 넘는 경우 마스크 생성에서 오류가 발생합니다.\n",
    "        # 실제로는 forward 메서드에 들어오기 전에 LLM이 입력이 `context_length`를\n",
    "        # 넘지 않는지 확인하기 때문에 문제가 되지 않습니다.\n",
    "\n",
    "        keys = self.W_key(x) # 크기: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # `num_heads` 차원을 추가함으로써 암묵적으로 행렬을 분할합니다.\n",
    "        # 그다음 마지막 차원을 `num_heads`에 맞춰 채웁니다: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 전치: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # 코잘 마스크로 스케일드 점곱 어텐션(셀프 어텐션)을 계산합니다.\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # 각 헤드에 대해 점곱을 수행합니다.\n",
    "\n",
    "        # 마스크를 불리언 타입으로 만들고 토큰 개수로 마스크를 자릅니다.\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # 마스크를 사용해 어텐션 점수를 채웁니다.\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 크기: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # 헤드를 결합합니다. self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # 투영\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcHAfDrVAeRm",
    "outputId": "dcf3bfe6-d028-4f36-afe6-3d25e11bebc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
    "# a.shape -> (1, 2, 3, 4) batch = 1, num_heads = 2, num_tokens = 3, head_dim = 4, 각 head마다 “토큰 3개, 각 토큰의 벡터는 길이 4”인 구조.\n",
    "\n",
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "print(a @ a.transpose(2, 3)) # [1, 2, 3, 3]\n",
    "# a.transpose(2, 3) = (1, 2, 3, 4) -> (1, 2, 4, 3), num_tokens ↔ head_dim 축을 바꿔버림\n",
    "# @ 연산은 마지막 두 차원만 가지고 행렬곱을 수행하고, 나머지 차원(b, head)은 그대로 유지하면서 독립적으로 계산한다. 마지막 두 차원 앞에 있는 배치 차원들은 자동 브로드캐스팅\n",
    "# transpose(2, 3) == transpose(-2, -1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7HM7drGAfVU",
    "outputId": "d1c2a5f8-bf5f-429e-eb37-8dfeef8e631f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 헤드:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "두 번째 헤드:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n"
     ]
    }
   ],
   "source": [
    "# 각 head별 attention score(핵심 연산(QKᵀ))\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"첫 번째 헤드:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\n두 번째 헤드:\\n\", second_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sApZtKgpAgVE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNttsoBReltEj9ho547BhqA",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
