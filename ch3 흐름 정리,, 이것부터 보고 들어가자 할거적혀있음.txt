3장 흐름도 정리해서 md 처음에 #으로 계시,, 
노션에 있는 개념정리랑 + git에 있는 노트 개념정리도,,
하면서 그 책 설명되어있는 pdf 같이한번 체크해야해요
3장 손으로 적어서 이해?

전체 흐름도 먼저 적기
2샘플 시퀀스 6개 토큰에 3차원 벡터로 임베딩한 (6,3)행렬
3self attention 설명 및 attention score 구하기
input시퀀스의 첫번째 토큰 샘플에 대해 as 구하기
쿼리 벡터, 키 벡터 모두 input의 x로 가정, 가중치 안곱하고
4
5 attetion score 정규화1
6. 정규화2
7. 정규화3 소트프맥스
8. input1 쿼리 샘플에 attentionweight * v 해서 문맥벡터 구하기
9. 모든 입력 토큰 쌍에 대해 attention score행렬를 계산
10. 모든 입력 토큰 쌍에 대해 접곱 통해attention score 행렬를 계산
11. 각 행 softmax 정규화
12. 
13. 정규화된 attention weight 행렬에 v(input) 점곱통해 문맥벡터
16. 입력임베딩을 통해 각각의 벡터가 만들어진다.
이때 벡터의 출력이 보통은 입력임베딩의 크기와 같지만
실험을 위해 입력임베딩은 3 벡터 출력2 로 실험
은각각의 벡터를 만들기 위한 가중치
17. qkv -> x *wq, x*wk, x*wv 구현 시작 nn.parameter통해
18. 가중치벡터 크기 변환을 통해 출력의 크기 변환 확인
19 .가중치벡터 크기 변환을 통해 출력의 크기 변환 확인
20. 1차원 텐서만 dot 사용, 1차원이라 전치사용안함
23. 어텐션점수를 가중치로 변환할때 
키 벡터의 차원의 제곱근으로 나눈후 진행한다.  
여기 노션에 마지막 두개 노트 같이 정리하면 좋아용
24. 출력 차원 변한거 적용되는거확인
25. parameter 사용해 배치 개념없는 싱글헤트어텐션(셀프어텐션)
26. .t 말고 transpose 사용
27. linear 사용 selfattetion v2 parameter와 차이

28. 코잘 어텐션 개념 추가, 미래위치 단어 숨기기
어텐션 스코어는 정방행렬,,  
29. # 행(row, i) = 현재 "쿼리 토큰" (i번째 토큰)
# 열(column, j) = 보고 싶은 "키 토큰" (j번째 토큰)
이게 핵심,, 위에 가리는 코잘 어텐션 개념
30. 코잘 마스킹 주대각선 위쪽 0 만드는 코드
31. 코잘 마스킹 주대각선 위쪽 0 만들고 해야하는 다음작업
각 행별로 남아있는 값들 합 1이되도록 정규화

32. 실무기준으로 다시, 코잘마스킹후 정규화 하기전 소프트맥스 먼저 돌리고
정규화하는 과정
33. 이후 소프트맥스로 정규화 및,, dk벡터차원제곱근나누기도 가이진행
dk제곱근으로 나누는것은 스케일링이라 부르고 Scaled Dot-Product Attention 미리 수치 깎아주는 사전 정규화
softmax를 취하는 것은 확률 분포화 개념의 정규화
둘 다 넓은 의미의 정규화가 맞습니다. **$\sqrt{d_k}$**는 Softmax가 제 기능을 하도록 돕는 '수치 안정화(Numerical Stability)' 장치이고, Softmax는 최종적인 '가중치 결정' 장치라고 이해하시면 완벽합니다.

34. 과적합방지위한 dropout 설명 softmax이후에 적용
35. 드롭아웃 날리고 나머지 스케일 조정 이유까지 설명
36. 구한 가중치에적용
37. stack은 기존에 있던 것들을 '포개서' 새로운 층(차원)을 하나 더 만드는 명령어이기 때문입니다.
포개기
inputs: 데이터가 적힌 종이 한 장이라고 생각하세요. (크기: 6행 3열)

torch.stack((inputs, inputs), ...): 똑같은 종이 두 장을 준비해서 겹치겠다는 뜻입니다.

dim=0: "맨 앞에 새로운 축을 만들어서 쌓아라"라는 뜻입니다.
stack 함수에 넣어준 텐서의 개수가 2개였기 때문입니다.

만약 torch.stack((inputs, inputs, inputs), dim=0) 이라고 썼다면?

결과는 torch.Size([3, 6, 3])이 됩니다. (3장을 쌓았으니까요!)
. cat (Concatenate) 과의 결정적 차이이게 가장 헷갈리는 부분인데, stack과 cat을 비교하면 확실해집니다.stack (쌓기): 새로운 차원을 만듭니다.[6, 3] 과 [6, 3]을 stack하면 $\rightarrow$ [2, 6, 3] (새로운 층이 생김)cat (이어붙이기): 기존 차원에 그대로 이어 붙입니다.torch.cat((inputs, inputs), dim=0)을 하면 $\rightarrow$ [12, 3] (세로로 길어짐)
38. 배치가 포함된 싱글헤드어텐션 -> 여기까지를 causal attention이라하나?그건아니고 그저 
의도: "과거와 현재 데이터만 써서 다음을 맞히겠다."

수단: "어텐션 맵에 **삼각형 모양의 마스크(Triangular Mask)**를 씌우겠다."

정체성: "나는 Decoder(디코더) 기반 모델이다." (예: GPT) 란뜼
39. 멀티헤드 개념추가된 함수 각 해드별 for문통해 구하고 cat한다 
실무에서는 cat아 아니였음 -> 노션정리된거확인
40. 39랑 차이는 병렬적으로 수행하는거같은데 이거 확인,,
41 transpose 문법 개념
42. # 각 head별 attention score(핵심 연산(QKᵀ))